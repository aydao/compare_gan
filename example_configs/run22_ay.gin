# Parameters for AdamOptimizer:
# ==============================================================================
tf.train.AdamOptimizer.beta1 = 0.0
tf.train.AdamOptimizer.beta2 = 0.999
tf.train.AdamOptimizer.epsilon = 1e-08
tf.train.AdamOptimizer.use_locking = False

# Parameters for batch_norm:
# ==============================================================================
# None.

# Parameters for BigGanResNetBlock:
# ==============================================================================
BigGanResNetBlock.add_shortcut = True

# Parameters for conditional_batch_norm:
# ==============================================================================
conditional_batch_norm.use_bias = False

# Parameters for cross_replica_moments:
# ==============================================================================
cross_replica_moments.group_size = None
cross_replica_moments.parallel = True

# Parameters for D:
# ==============================================================================
D.batch_norm_fn = None
D.layer_norm = False
D.spectral_norm = True

# Parameters for dataset:
# ==============================================================================
dataset.name = "danbooru_512"
dataset.seed = 777

# Parameters for resnet_biggan.Discriminator:
# ==============================================================================
resnet_biggan.Discriminator.blocks_with_attention = '64'
resnet_biggan.Discriminator.ch = 128
resnet_biggan.Discriminator.project_y = False

# Parameters for G:
# ==============================================================================
G.batch_norm_fn = @batch_norm
G.spectral_norm = True

# Parameters for resnet_biggan.Generator:
# ==============================================================================
resnet_biggan.Generator.blocks_with_attention = '64'
resnet_biggan.Generator.ch = 128
resnet_biggan.Generator.embed_bias = False
resnet_biggan.Generator.embed_y = False
resnet_biggan.Generator.embed_y_dim = 128
resnet_biggan.Generator.embed_z = False
resnet_biggan.Generator.hierarchical_z = True
### resnet_biggan.Generator.experimental_fast_conv_to_rgb = False

# Parameters for hinge:
# ==============================================================================
# None.

# Parameters for loss:
# ==============================================================================
loss.fn = @hinge

# Parameters for ModularGAN:
# ==============================================================================
ModularGAN.conditional = False
ModularGAN.deprecated_split_disc_calls = False
ModularGAN.ema_decay = 0.9999
ModularGAN.ema_start_step = 40000
ModularGAN.experimental_force_graph_unroll = False
ModularGAN.experimental_joint_gen_for_disc = False
ModularGAN.fit_label_distribution = False
ModularGAN.g_use_ema = True

# Parameters for no_penalty:
# ==============================================================================
# None.

# Parameters for normal:
# ==============================================================================
normal.mean = 0.0
normal.seed = None

# Parameters for options:
# ==============================================================================
options.architecture = "resnet_biggan_arch"
options.batch_size = 1024
options.disc_iters = 2
options.discriminator_normalization = None
options.gan_class = @SSGAN
options.lamba = 1
options.training_steps = 250000
options.z_dim = 128

# Parameters for penalty:
# ==============================================================================
penalty.fn = @no_penalty

# Parameters for run_config:
# ==============================================================================
run_config.iterations_per_loop = 250
run_config.keep_checkpoint_max = 1000
run_config.save_checkpoints_steps = 500
run_config.single_core = False
run_config.tf_random_seed = None

# Parameters for spectral_norm:
# ==============================================================================
spectral_norm.singular_value = "auto"
spectral_norm.epsilon = 1e-12

# Parameters for SSGAN:
# ==============================================================================
SSGAN.d_lr = 0.0004 # verify if using SSGAN instead
SSGAN.d_optimizer_fn = @tf.train.AdamOptimizer
SSGAN.g_lr = 5e-05  # verify if using SSGAN instead
SSGAN.g_optimizer_fn = @tf.train.AdamOptimizer
SSGAN.rotated_batch_size = 1024
SSGAN.self_supervision = 'rotation_gan'
SSGAN.weight_rotation_loss_d = 1.0
SSGAN.weight_rotation_loss_g = 0.2

# Parameters for standardize_batch:
# ==============================================================================
standardize_batch.decay = 0.9
standardize_batch.epsilon = 1e-05
standardize_batch.use_cross_replica_mean = None # NOTE: None is _True_ for TPUs 
standardize_batch.use_moving_averages = False

# Parameters for weights:
# ==============================================================================
weights.initializer = "orthogonal"

# Parameters for z:
# ==============================================================================
z.distribution_fn = @tf.random.normal
z.maxval = 1.0
z.minval = -1.0
z.stddev = 1.0
